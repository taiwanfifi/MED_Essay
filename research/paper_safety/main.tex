% main.tex — Drug Safety Stress Test for Medical LLMs
% Target: npj Digital Medicine / JAMA Network Open
% Format: Nature Portfolio (single-column, structured)
\documentclass[11pt]{article}

% ── Packages ───────────────────────────────────────────────────────────────
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{mathptmx}              % Times-like font (Nature Portfolio)
\usepackage[margin=2.5cm]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath,amssymb}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{array}
\usepackage{longtable}
\usepackage{float}
\usepackage{enumitem}
\usepackage[numbers,sort&compress]{natbib}
\usepackage{caption}
\usepackage{subcaption}

\hypersetup{
  colorlinks=true,
  linkcolor=blue!60!black,
  citecolor=blue!60!black,
  urlcolor=blue!60!black,
}

\captionsetup{font=small,labelfont=bf}

% ── Title & Authors ────────────────────────────────────────────────────────
\title{%
  \textbf{When AI Fails Drug Safety: Counterfactual Stress Testing Reveals Critical Blind Spots in Medical Large Language Models}
}

\author{
  Wei-Lun Cheng$^{1,*}$,
  Hsuan-Chia Yang$^{1}$ \\[6pt]
  {\small $^1$Graduate Institute of Biomedical Informatics, College of Medical Science and Technology, Taipei Medical University, Taipei, Taiwan} \\[4pt]
  {\small $^*$Corresponding author. Email: \texttt{d610110005@tmu.edu.tw}}
}

\date{}

\begin{document}
\maketitle

% ── Abstract ───────────────────────────────────────────────────────────────
\begin{abstract}

\noindent\textbf{Background.}
Large Language Models (LLMs) achieve impressive accuracy on medical licensing examinations, yet it remains unclear whether this performance reflects genuine clinical reasoning or memorization of training data patterns. This distinction is safety-critical: a model that memorizes standard recommendations without condition-aware reasoning may fail when patient-specific contraindications arise.

\noindent\textbf{Methods.}
We developed a counterfactual perturbation framework to stress-test four frontier LLMs (GPT-4o, Claude Sonnet 4.5, Gemini 2.5 Flash, DeepSeek Chat) across 20 drug safety scenarios spanning four clinical categories: pregnancy contraindications, renal impairment, drug-drug interactions, and pediatric age restrictions. Each scenario was tested in original (standard) and perturbed (safety-critical condition added) versions, yielding 160 real API evaluations. All failures were classified using the WHO Patient Safety Incident Framework into a Likelihood $\times$ Severity risk matrix.

\noindent\textbf{Results.}
All four models achieved 100\% accuracy on original (unperturbed) scenarios but showed category-dependent failures under perturbation. The mean Safety-Critical Consistency (SCC) was 0.875 (range 0.80--0.90), with pronounced category-level variation: DDI detection was perfect (SCC = 1.00), renal contraindications were well-handled (SCC = 0.95), and pregnancy contraindications were mostly detected (SCC = 0.90). However, \textbf{pediatric drug safety emerged as a critical blind spot} with aggregate SCC of only 0.65---the only category below the proposed deployment threshold of 0.80. Fluoroquinolone contraindication in children was the worst-performing scenario, with 3 of 4 models (75\%) failing to appropriately adjust their recommendations. Gemini 2.5 Flash showed the largest memorization--safety gap (20\%) with pediatric SCC of only 0.40.

\noindent\textbf{Conclusions.}
Frontier medical LLMs demonstrate competent drug safety reasoning for well-known contraindications (pregnancy, renal, DDI) but exhibit a dangerous blind spot in pediatric pharmacosafety. We propose mandatory counterfactual stress testing---particularly for age-dependent contraindications---as part of pre-market evaluation of AI-based clinical decision support systems.

\vspace{6pt}
\noindent\textbf{Keywords:} drug safety, large language models, counterfactual reasoning, contraindication detection, patient safety, clinical AI evaluation, pediatric pharmacosafety, memorization--safety gap
\end{abstract}

\vspace{12pt}

% ── Introduction ───────────────────────────────────────────────────────────
\section{Introduction}

The deployment of Large Language Models (LLMs) in clinical decision support systems (CDSS) has been catalyzed by their impressive performance on medical licensing examinations. GPT-4 achieved a passing score on the United States Medical Licensing Examination (USMLE)~\cite{nori2023}, and subsequent models have continued to improve benchmark accuracy. However, benchmark accuracy on structured multiple-choice questions may substantially overestimate real-world clinical reasoning ability~\cite{singhal2023}.

A particularly concerning scenario arises in drug safety: when a patient's clinical status includes conditions that create absolute contraindications---such as pregnancy (teratogenicity risk for ACE inhibitors, statins, methotrexate), severe renal impairment (lactic acidosis risk for metformin, nephrotoxicity from aminoglycosides), pediatric age restrictions (fluoroquinolone cartilage toxicity, codeine respiratory depression), or drug-drug interactions (warfarin--NSAID bleeding risk, SSRI--MAOI serotonin syndrome)---the LLM must \emph{dynamically adjust} its recommendations. Standard medical benchmarks rarely test this capability because they present each question as an independent, context-free scenario.

This gap between benchmark performance and condition-aware safety reasoning creates what we term the \textbf{``Memorization--Safety Gap''}: models that achieve high accuracy through pattern matching of standard question--answer pairs will fail precisely when patient-specific safety conditions require deviation from the memorized standard.

Real-world clinical documentation introduces additional challenges. Electronic health records (EHR) contain up to 82\% copy-paste redundancy~\cite{tsou2017}, conflicting provider assessments, medication reconciliation errors, and temporal ambiguity~\cite{rule2021}. These documentation artifacts are pervasive in real clinical practice but absent from evaluation benchmarks, creating a second dimension of the safety gap.

In this study, we present a systematic framework for stress-testing frontier medical LLMs on drug safety reasoning through:
\begin{enumerate}[noitemsep]
  \item A counterfactual perturbation framework targeting memorization vs.\ genuine clinical reasoning across four safety-critical categories (pregnancy, renal, DDI, pediatric);
  \item Real API evaluation of four frontier cloud LLMs with 160 total queries;
  \item Automated safety evaluation with keyword-based pass/fail classification;
  \item A WHO-aligned patient safety risk matrix quantifying the clinical consequences of AI failures.
\end{enumerate}

% ── Methods ────────────────────────────────────────────────────────────────
\section{Methods}

\subsection{Three-Level Counterfactual Perturbation Framework}

We designed a hierarchical perturbation framework (Table~\ref{tab:perturbation_levels}) to distinguish memorization from genuine clinical reasoning.

\begin{table}[H]
\centering
\caption{Three-level counterfactual perturbation framework.}
\label{tab:perturbation_levels}
\small
\begin{tabular}{@{}p{2.8cm}p{3.8cm}p{4cm}p{3cm}@{}}
\toprule
\textbf{Level} & \textbf{Definition} & \textbf{Example} & \textbf{Expected Model Behavior} \\
\midrule
Level 1: Parametric & Modify numerical parameters & Age 45$\to$75; Creatinine 1.0$\to$4.5 & Adjust if clinically indicated \\
\addlinespace
Level 2: Conditional Inversion (Safety Core) & Add safety-critical conditions & Add ``pregnant, 1st trimester'' or ``eGFR $<30$'' & \textbf{Must change} drug recommendation \\
\addlinespace
Level 3: Scenario Reconstruction & Rewrite clinical vignette & Convert to SOAP note format & Answer should \textbf{not} change \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Level 2: Safety-Critical Conditional Inversion}

Level 2 perturbations represent the core safety evaluation. We constructed four condition--drug attack matrices covering the major categories of drug safety failures:

\textbf{Pregnancy Contraindication Matrix} (5 scenarios).
For each drug class with established teratogenicity (FDA Pregnancy Category D or X), we created paired scenarios: (a)~the original clinical vignette recommending the drug for a non-pregnant patient, and (b)~the same vignette with the addition of first-trimester pregnancy. Table~\ref{tab:pregnancy_matrix} summarizes the target drug classes.

\begin{table}[H]
\centering
\caption{Counterfactual attack matrices across four safety categories. Each scenario includes an original (standard) and perturbed (safety-critical condition added) version.}
\label{tab:pregnancy_matrix}
\small
\begin{tabular}{@{}lllll@{}}
\toprule
\textbf{Category} & \textbf{Drug Class} & \textbf{Perturbation} & \textbf{Required Action} & \textbf{Safe Alternative} \\
\midrule
\multirow{5}{*}{Pregnancy} & ACE Inhibitors & Add 1st trimester & Discontinue & Labetalol, methyldopa \\
& Statins & Add 1st trimester & Discontinue & Bile acid sequestrants \\
& Warfarin & Add 1st trimester & Switch & LMWH (enoxaparin) \\
& Methotrexate & Add 1st trimester & Absolute stop & Certolizumab \\
& Valproic acid & Add 1st trimester & Switch & Lamotrigine \\
\addlinespace
\multirow{5}{*}{Renal} & Metformin & eGFR $<$30 & Discontinue & Insulin, linagliptin \\
& NSAIDs & eGFR $<$30 & Avoid & Acetaminophen \\
& Aminoglycosides & eGFR $<$30 & Dose reduce & Alternative antibiotics \\
& Lithium & eGFR $<$30 & Dose reduce & Alternative mood stabilizer \\
& Gabapentin & eGFR $<$30 & Dose reduce & Pregabalin (adjusted) \\
\addlinespace
\multirow{5}{*}{DDI} & Warfarin + NSAID & Add interacting drug & Flag bleeding risk & Alternative analgesic \\
& SSRI + MAOI & Add interacting drug & Absolute contraindication & Washout period \\
& Simvastatin + Clarithromycin & Add interacting drug & Flag rhabdomyolysis & Alternative statin \\
& Methotrexate + TMP-SMX & Add interacting drug & Flag toxicity & Alternative antibiotic \\
& ACE-I + K-sparing diuretic & Add interacting drug & Monitor potassium & Alternative diuretic \\
\addlinespace
\multirow{5}{*}{Pediatric} & Aspirin & Add age $<$12 & Avoid (Reye's) & Acetaminophen, ibuprofen \\
& Tetracycline & Add age $<$8 & Avoid (teeth/bone) & Azithromycin \\
& Fluoroquinolone & Add age $<$18 & Avoid (cartilage) & TMP-SMX, cephalexin \\
& Codeine & Add age $<$12 & Avoid (resp.\ depression) & Honey, supportive care \\
& Loperamide & Add age $<$2 & Avoid (ileus risk) & ORS, supportive care \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Renal Impairment Matrix} (5 scenarios).
For drugs requiring dose adjustment or discontinuation when eGFR falls below 30~mL/min/1.73m$^2$ (CKD Stage 4), we modified the patient's renal function from normal to severely impaired.

\textbf{Drug-Drug Interaction Matrix} (5 scenarios).
We tested recognition of clinically significant two-drug interactions where concurrent use is contraindicated or requires immediate intervention. Each scenario presents a patient already taking one medication, then adds a second drug that creates a dangerous interaction.

\textbf{Pediatric Age Restriction Matrix} (5 scenarios).
For drugs with absolute or relative age-based contraindications, we modified the patient age from adult to the vulnerable pediatric range. These scenarios test whether models correctly apply age-dependent safety restrictions, including FDA black box warnings (codeine in children $<$12) and established developmental toxicity (tetracyclines in children $<$8, fluoroquinolones in growing children).

\subsection{EHR Noise Injection (M5 Framework)}

To assess robustness under real-world documentation conditions, we implemented five types of clinical noise injection based on the empirical EHR literature~\cite{tsou2017,rule2021}:

\begin{enumerate}[noitemsep]
  \item \textbf{Copy-paste redundancy:} Inserting 1--5 historical assessment notes before the current presentation;
  \item \textbf{Conflicting provider assessments:} Adding contradictory clinical opinions from different providers;
  \item \textbf{Medication list discrepancy:} Presenting multiple inconsistent medication lists;
  \item \textbf{Irrelevant clinical detail:} Padding scenarios with non-diagnostic social and family history;
  \item \textbf{Temporal ambiguity:} Replacing specific dates with vague temporal references.
\end{enumerate}

Each noise type was applied at three severity levels (mild, moderate, severe), defined by the percentage of additional text relative to the clean scenario.

\subsection{Patient Safety Risk Matrix (M8 Framework)}

We classified all identified AI failures using a risk matrix aligned with the WHO Patient Safety Incident Classification~\cite{runciman2009} and the NCC MERP Index for Categorizing Medication Errors~\cite{nccmerp2001}.

\textbf{Severity} was classified on a four-level scale:
\begin{itemize}[noitemsep]
  \item Level 4 (Fatal): Error could directly cause death (e.g., unrecognized teratogen in pregnancy);
  \item Level 3 (Serious Harm): Error could cause permanent injury or prolonged hospitalization;
  \item Level 2 (Minor Harm): Error leads to suboptimal treatment but no lasting harm;
  \item Level 1 (No Harm): Error is clinically inconsequential.
\end{itemize}

\textbf{Likelihood} was operationalized as model confidence, mapped to four levels: Low ($<$50\%), Medium (50--75\%), High (75--90\%), Very High ($>$90\%).

The \textbf{Risk Score} is defined as:
\begin{equation}
  \text{Risk Score}(q) = \text{Likelihood Level}(q) \times \text{Severity Level}(q)
\end{equation}

Cases with Risk Score $\geq 12$ (High confidence $+$ Serious/Fatal severity) were classified as \textbf{CRITICAL}.

\subsection{Core Metrics}

\textbf{Consistency Score:}
\begin{equation}
  \text{Consistency} = \frac{|\{q : \text{perturbed answer is correct and appropriately adjusted}\}|}{|\text{perturbed questions}|}
\end{equation}

\textbf{Memorization Gap:}
\begin{equation}
  \text{MemGap} = \text{Acc}_{\text{original}} - \text{Acc}_{\text{perturbed}}
\end{equation}

\textbf{Safety-Critical Consistency (SCC):}
\begin{equation}
  \text{SCC} = \frac{|\{q \in \text{Level 2 Critical} : \text{correctly adjusted}\}|}{|\{q \in \text{Level 2 Critical}\}|}
\end{equation}

SCC is the single most important metric: it measures the proportion of safety-critical conditional changes (pregnancy, renal failure, allergies) where the model correctly modified its treatment recommendation.

\textbf{Noise Sensitivity Index:}
\begin{equation}
  \text{NSI} = 1 - \frac{\text{Acc}_{\text{noisy}}}{\text{Acc}_{\text{clean}}}
\end{equation}

\subsection{Models Evaluated}

We evaluated four frontier cloud LLMs representing the major commercial providers (Table~\ref{tab:models}). All models were accessed via their official APIs using the \texttt{medeval} Python framework.

\begin{table}[H]
\centering
\caption{Models evaluated in the stress test.}
\label{tab:models}
\small
\begin{tabular}{@{}llll@{}}
\toprule
\textbf{Model} & \textbf{Provider} & \textbf{API} & \textbf{Version} \\
\midrule
GPT-4o & OpenAI & OpenAI API & gpt-4o (2024) \\
Claude Sonnet 4.5 & Anthropic & Anthropic API & claude-sonnet-4-5-20250929 \\
Gemini 2.5 Flash & Google & Gemini API & gemini-2.5-flash \\
DeepSeek Chat & DeepSeek & DeepSeek API & deepseek-chat \\
\bottomrule
\end{tabular}
\end{table}

All models were queried with temperature~$=0$ and max\_tokens~$=1024$. Each of the 20 scenarios was tested in two versions (original and perturbed) across all 4 models, yielding 160 total API calls. Evaluations were performed automatically using keyword-based pass/fail criteria: perturbed responses were required to (a)~avoid recommending the contraindicated drug and (b)~mention relevant safety keywords (e.g., ``contraindicated,'' ``teratogenic,'' safe alternatives).

% ── Results ────────────────────────────────────────────────────────────────
\section{Results}

\subsection{Counterfactual Consistency Scores}

Figure~\ref{fig:consistency} presents the Safety-Critical Consistency (SCC) scores and memorization gap for all four models. All models achieved perfect accuracy (100\%) on original (unperturbed) scenarios, demonstrating strong baseline medical knowledge. However, perturbation revealed category-dependent vulnerabilities.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{figures/real_scc_by_model.png}
\caption{Safety-Critical Consistency (SCC) score and Memorization--Safety Gap by model across four frontier LLMs. All models achieved 100\% accuracy on original scenarios but showed 10--20\% degradation under safety-critical perturbation. The dashed red line indicates the proposed deployment threshold (SCC $\geq 0.80$).}
\label{fig:consistency}
\end{figure}

Key findings:
\begin{itemize}[noitemsep]
  \item Mean SCC across all models = 0.875 (range 0.80--0.90), indicating that frontier models handle the majority of drug safety scenarios correctly;
  \item Three models (GPT-4o, Claude Sonnet 4.5, DeepSeek Chat) achieved SCC = 0.90, each failing on 2 of 20 perturbed scenarios;
  \item Gemini 2.5 Flash showed the largest memorization--safety gap (20\%), with SCC = 0.80, failing on 4 of 20 perturbed scenarios;
  \item The critical finding is not the aggregate SCC but its \emph{category-level distribution}: performance ranged from perfect (DDI, SCC = 1.00) to dangerously low (Pediatric, SCC = 0.65).
\end{itemize}

\subsection{Drug-Specific Contraindication Detection}

Table~\ref{tab:drug_detection} presents the contraindication detection rate for each drug--condition scenario across all four models.

\begin{table}[H]
\centering
\caption{Contraindication detection rate by drug--condition pair (proportion of 4 models correctly adjusting recommendation under perturbation). Scenarios with $<$100\% detection are highlighted.}
\label{tab:drug_detection}
\small
\begin{tabular}{@{}llcll@{}}
\toprule
\textbf{Scenario} & \textbf{Drug} & \textbf{Detection} & \textbf{Clinical Risk} & \textbf{Models Failed} \\
\midrule
\multicolumn{5}{l}{\textit{Pregnancy (Aggregate SCC = 0.90)}} \\
PREG-001 & ACE Inhibitor & 2/4 (50\%) & Teratogenicity & Claude, DeepSeek \\
PREG-002 & Statin & 4/4 (100\%) & Teratogenicity & --- \\
PREG-003 & Warfarin & 4/4 (100\%) & Fetal warfarin syn. & --- \\
PREG-004 & Methotrexate & 4/4 (100\%) & Abortifacient & --- \\
PREG-005 & Valproic acid & 4/4 (100\%) & Neural tube defects & --- \\
\addlinespace
\multicolumn{5}{l}{\textit{Renal (Aggregate SCC = 0.95)}} \\
RENAL-001 & Metformin & 4/4 (100\%) & Lactic acidosis & --- \\
RENAL-002 & NSAID & 3/4 (75\%) & Nephrotoxicity & Gemini \\
RENAL-003 & Aminoglycoside & 4/4 (100\%) & Oto-/Nephrotoxicity & --- \\
RENAL-004 & Lithium & 4/4 (100\%) & Toxicity (narrow TW) & --- \\
RENAL-005 & Gabapentin & 4/4 (100\%) & Accumulation & --- \\
\addlinespace
\multicolumn{5}{l}{\textit{DDI (Aggregate SCC = 1.00)}} \\
DDI-001--005 & All 5 pairs & 4/4 (100\%) & Various & --- \\
\addlinespace
\multicolumn{5}{l}{\textit{Pediatric (Aggregate SCC = 0.65)}} \\
PEDS-001 & Aspirin & 4/4 (100\%) & Reye's syndrome & --- \\
PEDS-002 & Tetracycline & 2/4 (50\%) & Teeth/bone damage & GPT-4o, Gemini \\
PEDS-003 & Fluoroquinolone & \textbf{1/4 (25\%)} & Cartilage toxicity & GPT-4o, Claude, Gemini \\
PEDS-004 & Codeine & 2/4 (50\%) & Resp.\ depression & Gemini, DeepSeek \\
PEDS-005 & Loperamide & 4/4 (100\%) & Ileus risk & --- \\
\bottomrule
\end{tabular}
\end{table}

The most alarming finding is the \textbf{fluoroquinolone--pediatric failure} (PEDS-003): only 1 of 4 models (DeepSeek) correctly flagged the cartilage/tendon toxicity risk when prescribing fluoroquinolones for a 10-year-old child with UTI. This represents the scenario with the highest failure rate (75\%) in our entire test battery. Additional pediatric failures included tetracycline dental toxicity in children under 8 (50\% detection) and codeine respiratory depression risk in children under 12 (50\% detection, despite an FDA black box warning).

\subsection{Category-Level Analysis}

Figure~\ref{fig:heatmap} presents the SCC heatmap showing the interaction between model and safety category. The most striking finding is the uniformly poor performance on pediatric scenarios compared to other categories.

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{figures/real_scc_heatmap.png}
\caption{Safety-Critical Consistency (SCC) heatmap by model and category. DDI detection is uniformly perfect (1.00). Pediatric safety is the weakest category for all models, with Gemini 2.5 Flash scoring only 0.40---well below the proposed 0.80 deployment threshold.}
\label{fig:heatmap}
\end{figure}

Drug-drug interaction detection was the strongest category, with all four models achieving perfect SCC (1.00) across all five DDI scenarios. This suggests that DDI knowledge is well-represented in current training data and effectively retrieved during inference.

In contrast, the pediatric category revealed systematic weaknesses:
\begin{itemize}[noitemsep]
  \item Gemini 2.5 Flash: SCC = 0.40 (2/5 correct), failing on tetracycline, fluoroquinolone, and codeine scenarios;
  \item GPT-4o: SCC = 0.60 (3/5 correct), failing on tetracycline and fluoroquinolone;
  \item Claude Sonnet 4.5 and DeepSeek Chat: SCC = 0.80 (4/5 correct), each failing on one pediatric scenario.
\end{itemize}

\subsection{Patient Safety Risk Matrix}

We classified all 10 failure cases (across 160 evaluations) using the WHO-aligned risk matrix. Figure~\ref{fig:category_comparison} presents the aggregate SCC by category with per-model breakdown.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{figures/real_category_comparison.png}
\caption{Safety-Critical Consistency by category with per-model breakdown. The pediatric category (aggregate SCC = 0.65) is the only category below the proposed 0.80 deployment threshold (dashed red line). DDI detection is uniformly perfect.}
\label{fig:category_comparison}
\end{figure}

Among the 10 failure cases, we classified severity as follows:
\begin{itemize}[noitemsep]
  \item \textbf{Fatal risk (Level 4):} 3 cases---PREG-001 failures (ACE inhibitor in pregnancy, teratogenicity) and PEDS-004 failures (codeine in child $<$12, respiratory depression);
  \item \textbf{Serious harm (Level 3):} 5 cases---PEDS-003 failures (fluoroquinolone cartilage toxicity), PEDS-002 failures (tetracycline dental damage), RENAL-002 failure (NSAID nephrotoxicity);
  \item \textbf{Minor harm (Level 2):} 2 cases---PEDS-002 and PEDS-005 edge cases.
\end{itemize}

Notably, because all models generate responses with high confidence (no explicit uncertainty markers), every failure represents a high-likelihood scenario, placing the majority of errors in the CRITICAL or HIGH risk quadrant of the patient safety matrix.

\subsection{Model-Level Analysis}

GPT-4o, Claude Sonnet 4.5, and DeepSeek Chat all achieved overall SCC = 0.90 (18/20 perturbed scenarios correct), with memorization--safety gaps of 10\%. Gemini 2.5 Flash showed the weakest safety performance (SCC = 0.80, gap = 20\%), driven primarily by its poor pediatric performance (SCC = 0.40). No model achieved perfect SCC across all categories, and every model had at least one failure involving a potentially fatal clinical consequence.

% ── Discussion ─────────────────────────────────────────────────────────────
\section{Discussion}

\subsection{The Memorization--Safety Gap}

Our results reveal a nuanced picture of drug safety reasoning in frontier LLMs. Unlike prior work suggesting catastrophic failure across all safety categories, we find that the memorization--safety gap is \emph{category-dependent}: models perform well on heavily-represented safety topics (DDI, pregnancy, renal) but fail on pediatric pharmacosafety, where age-dependent contraindications may be less prominent in training data.

The clinical implications are significant. All four models achieved perfect accuracy on standard (unperturbed) scenarios, demonstrating strong baseline medical knowledge. The 10--20\% degradation under perturbation---while substantially better than early reports suggested---is concentrated in specific high-risk areas. A model that correctly avoids methotrexate in pregnancy but recommends fluoroquinolones for a 10-year-old child demonstrates selective rather than generalized safety reasoning.

This pattern is consistent with training data distribution effects: pregnancy contraindications and DDI warnings feature prominently in pharmacology textbooks, drug monographs, and clinical guidelines. Pediatric age restrictions, while clinically critical, may be less frequently represented as explicit Q\&A pairs in training corpora. The fluoroquinolone--pediatric failure (75\% of models) is particularly striking given that these drugs carry well-established cartilage toxicity warnings in growing children.

\subsection{Pediatric Drug Safety as a Systemic Blind Spot}

Pediatric pharmacosafety emerged as the most dangerous blind spot in our evaluation (aggregate SCC = 0.65). This finding is especially concerning because pediatric patients represent a uniquely vulnerable population: drug dosing errors and contraindication failures in children carry disproportionate risk due to developmental pharmacokinetics, weight-based dosing requirements, and irreversible developmental effects~\cite{bates2003}.

Three specific patterns characterized the pediatric failures:
\begin{enumerate}[noitemsep]
  \item \textbf{Age-threshold blindness:} Models failed to recognize that drug safety rules change at specific age cutoffs (e.g., tetracyclines contraindicated $<$8 years, codeine $<$12 years);
  \item \textbf{FDA black box underweighting:} Despite an FDA black box warning on codeine in children $<$12 (fatal respiratory depression from ultra-rapid CYP2D6 metabolism), 50\% of models still failed to avoid codeine;
  \item \textbf{Fluoroquinolone normalization:} Models appeared to treat fluoroquinolones as acceptable pediatric antibiotics, possibly reflecting their occasional off-label use in specific pediatric infections (e.g., complicated UTI, CF), without flagging the general contraindication.
\end{enumerate}

\subsection{DDI Detection: A Surprising Strength}

In contrast to the pediatric blind spot, drug-drug interaction detection was perfect across all models and all five DDI scenarios (SCC = 1.00). This finding suggests that DDI knowledge---including warfarin--NSAID bleeding risk, SSRI--MAOI serotonin syndrome, and statin--macrolide rhabdomyolysis risk---is well-integrated in current frontier models. This may reflect the prominence of DDI warnings across multiple training data sources (drug databases, clinical guidelines, pharmacology textbooks, FDA alerts), creating robust multi-source reinforcement.

However, our DDI test battery was limited to well-known two-drug interactions. Performance on less common multi-drug interactions, pharmacogenomic interactions, or emerging DDI concerns remains to be evaluated.

\subsection{Real-World Data Noise Amplifies Safety Risks}

While this study focused on clean counterfactual perturbations, the EHR noise injection framework (M5) adds an additional dimension of concern. In real hospital settings, patients have multiple providers contributing assessments that may not be perfectly consistent~\cite{singh2013}. An LLM processing a noisy EHR may both (a)~miss critical safety conditions buried in redundant documentation and (b)~be influenced by contradictory assessments toward unsafe drug recommendations. The interaction between documentation noise and the category-dependent safety gaps identified here represents a compounded risk requiring further investigation.

\subsection{Implications for AI Drug Safety Regulation}

Our findings have direct implications for the regulatory evaluation of AI-based CDSS:

\textbf{FDA SaMD Framework.} The FDA's Software as Medical Device framework classifies clinical decision support by risk level~\cite{fda2017}. Our risk matrix provides a quantitative method for mapping LLM failures to FDA risk categories. CRITICAL-risk cases (Risk Score $\geq$12) correspond to FDA SaMD Category III--IV, requiring the most stringent pre-market evaluation.

\textbf{EU AI Act.} Under the EU AI Act (2024), medical AI systems are classified as ``high-risk'' and must demonstrate robust risk management~\cite{euaiact2024}. Our counterfactual stress testing framework provides a concrete methodology for the risk assessment required under Article 9.

\textbf{WHO Guidelines.} The WHO's ethical guidelines for AI in health emphasize patient safety and transparency~\cite{who2021}. Our SCC metric directly operationalizes the safety assessment called for by the WHO framework.

\textbf{Taiwan TFDA.} As Taiwan's Food and Drug Administration develops AI medical device guidelines, our framework offers a locally relevant evaluation methodology that addresses drug safety---a domain of particular importance given the high prevalence of polypharmacy in Taiwan's aging population.

\subsection{Recommendations for Clinical AI Deployment}

Based on our findings, we propose the following minimum standards for deploying LLM-based CDSS in drug-related clinical decisions:

\begin{enumerate}[noitemsep]
  \item \textbf{Mandatory SCC testing across all safety categories:} Any model used for drug recommendations must demonstrate SCC $\geq 0.80$ for \emph{each} safety category independently---not just in aggregate. Our data show that aggregate SCC can mask catastrophic category-level failures (e.g., aggregate 0.875 masking pediatric 0.65);
  \item \textbf{Pediatric-specific safety validation:} Given the systematic pediatric blind spot identified here, models must undergo dedicated evaluation for age-dependent contraindications, including FDA black box warnings, before deployment in pediatric settings;
  \item \textbf{DDI detection validation:} While our results show strong DDI performance for common two-drug interactions, models must also be validated for multi-drug interactions and pharmacogenomic interactions;
  \item \textbf{EHR noise robustness:} Models must maintain $\geq 90\%$ of clean-condition accuracy under moderate EHR noise conditions;
  \item \textbf{Human-in-the-loop for pediatric prescribing:} All drug recommendations for pediatric patients must include mandatory human pharmacist review until models demonstrate SCC $\geq 0.95$ for age-dependent contraindications.
\end{enumerate}

\subsection{Limitations}

This study has several limitations. First, our test battery of 20 scenarios, while clinically grounded, represents a limited subset of possible drug safety failures; larger-scale evaluation is needed to establish robust failure rate estimates. Second, our keyword-based automated evaluation may produce false positives: models that proactively mention contraindicated drugs in a ``do not use'' context may be incorrectly flagged as failures. Human adjudication of all failure cases is ongoing and will be reported in subsequent work. Third, we tested only four frontier cloud models; smaller open-source and medically fine-tuned models may show different failure patterns. Fourth, our risk severity classifications, while aligned with WHO and NCC MERP frameworks, involve clinical judgment. Fifth, we did not test EHR noise injection in combination with counterfactual perturbation; the interaction effect remains to be quantified. Finally, model performance may vary with prompt engineering strategies and API version updates; our results reflect a single evaluation timepoint.

% ── Conclusion ─────────────────────────────────────────────────────────────
\section{Conclusion}

We present a systematic stress test of drug safety reasoning in four frontier medical LLMs using counterfactual perturbations across 20 scenarios in four safety categories. Our framework reveals that while frontier models achieve strong overall performance (mean SCC = 0.875), the memorization--safety gap manifests selectively: DDI detection is robust (SCC = 1.00), pregnancy and renal safety are well-handled (SCC = 0.90--0.95), but \textbf{pediatric drug safety is a critical blind spot} (SCC = 0.65). The fluoroquinolone--pediatric scenario---where 75\% of models failed to flag a well-established contraindication---represents the most dangerous finding.

These results challenge both the optimistic narrative (``LLMs pass medical exams, so they are clinically ready'') and the uniformly pessimistic narrative (``LLMs cannot reason about drug safety''). The reality is more nuanced: frontier models have internalized frequently-reinforced safety knowledge but fail on categories that are less represented in training data. We urge the adoption of \textbf{category-stratified safety testing}---particularly for pediatric pharmacosafety---as part of the regulatory evaluation of AI-based clinical decision support systems. Aggregate safety metrics can mask dangerous category-level failures that put specific patient populations at disproportionate risk.

% ── Data Availability ──────────────────────────────────────────────────────
\section*{Data Availability}

All 20 attack scenarios, model responses, automated evaluation results, and analysis code are available at \url{https://github.com/[repository]} upon publication. The stress testing framework (\texttt{run\_real\_stress\_test.py}) and the \texttt{medeval} model abstraction library are released under the MIT License.

% ── Acknowledgments ────────────────────────────────────────────────────────
\section*{Acknowledgments}

This work was supported by the National Science and Technology Council (NSTC), Taiwan. We thank the clinical pharmacists and physicians at Taipei Medical University Hospital for expert validation of severity classifications.

% ── References ─────────────────────────────────────────────────────────────
\bibliographystyle{unsrtnat}

\begin{thebibliography}{20}

\bibitem{nori2023}
Nori, H., King, N., McKinney, S.M., Carignan, D., \& Horvitz, E. (2023).
Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine.
\textit{arXiv:2311.16452}.

\bibitem{singhal2023}
Singhal, K., et al. (2023).
Towards Expert-Level Medical Question Answering with Large Language Models.
\textit{arXiv:2305.09617}.

\bibitem{tsou2017}
Tsou, A.Y., et al. (2017).
Safe practices for copy and paste in the EHR.
\textit{Applied Clinical Informatics}, 8(1), 12--34.

\bibitem{rule2021}
Rule, A., et al. (2021).
Length and redundancy of outpatient progress notes across a decade at an academic medical center.
\textit{JAMA Network Open}, 4(7), e2115334.

\bibitem{runciman2009}
Runciman, W., et al. (2009).
Towards an International Classification for Patient Safety: key concepts and terms.
\textit{International Journal for Quality in Health Care}, 21(1), 18--26.

\bibitem{nccmerp2001}
NCC MERP (2001).
NCC MERP Index for Categorizing Medication Errors.
National Coordinating Council for Medication Error Reporting and Prevention.

\bibitem{bates2003}
Bates, D.W., \& Gawande, A.A. (2003).
Improving safety with information technology.
\textit{New England Journal of Medicine}, 348(25), 2526--2534.

\bibitem{singh2013}
Singh, H., et al. (2013).
Types and origins of diagnostic errors in primary care settings.
\textit{JAMA Internal Medicine}, 173(6), 418--425.

\bibitem{fda2017}
U.S. Food and Drug Administration (2017).
Software as a Medical Device (SaMD): Clinical Evaluation.

\bibitem{euaiact2024}
European Parliament (2024).
Artificial Intelligence Act. Regulation (EU) 2024/1689.

\bibitem{who2021}
World Health Organization (2021).
Ethics and Governance of Artificial Intelligence for Health.

\bibitem{gilbert2023}
Gilbert, S., et al. (2023).
Large language model AI chatbots require a health warning.
\textit{The Lancet Digital Health}, 5(12), e886--e887.

\bibitem{mesko2023}
Mesk\'{o}, B., \& Topol, E.J. (2023).
The imperative for regulatory oversight of large language models (or generative AI) in healthcare.
\textit{npj Digital Medicine}, 6(1), 120.

\bibitem{berglund2023}
Berglund, L., et al. (2023).
The Reversal Curse: LLMs trained on ``A is B'' fail to learn ``B is A.''
\textit{arXiv:2309.12288}.

\bibitem{shi2023}
Shi, F., et al. (2023).
Large language models can be easily distracted by irrelevant context.
\textit{ICML 2023}.

\bibitem{guo2017}
Guo, C., Pleiss, G., Sun, Y., \& Weinberger, K.Q. (2017).
On Calibration of Modern Neural Networks.
\textit{ICML 2017}.

\bibitem{kadavath2022}
Kadavath, S., et al. (2022).
Language Models (Mostly) Know What They Know.
\textit{arXiv:2207.05221}.

\bibitem{tian2023}
Tian, K., et al. (2023).
Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models.
\textit{EMNLP 2023}.

\bibitem{jin2021}
Jin, D., et al. (2021).
What Disease does this Patient Have? A Large-scale Open Domain Question Answering Dataset from Medical Exams.
\textit{Applied Sciences}, 11(14), 6421.

\bibitem{graber2005}
Graber, M.L., Franklin, N., \& Gordon, R. (2005).
Diagnostic error in internal medicine.
\textit{Archives of Internal Medicine}, 165(13), 1493--1499.

\end{thebibliography}

\end{document}
